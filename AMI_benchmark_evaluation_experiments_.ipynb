{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Allarwa/AMI-Dataset/blob/main/AMI_benchmark_evaluation_experiments_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LR3G-Vx12xP"
      },
      "source": [
        "# AMI benchmark experiments\n",
        "\n",
        "\n",
        "\n",
        "* Traditional\n",
        " * TF/IDF\n",
        " *  BOW\n",
        "* Word Embeddings\n",
        " * Word2Vec - CBoW\n",
        "\n",
        "* Language Models\n",
        " * AraBERT\n",
        "* DNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATYlOU1SkJRH"
      },
      "source": [
        "## Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTrf1juDaFH_"
      },
      "outputs": [],
      "source": [
        "!pip install -q tqdm\n",
        "!pip install -q joblib==1.1.0\n",
        "!pip install -q pyarabic\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import random"
      ],
      "metadata": {
        "id": "KJV6WjZSaiTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1u1qrk06JYE"
      },
      "source": [
        "## Mount your Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ0xVIXK59G_",
        "outputId": "3bb3e757-6b8b-4f68-e0cc-c385f71f4952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl_BiwR4kQE2"
      },
      "source": [
        "## Read the Dataset and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83HF_nqU9AVq"
      },
      "outputs": [],
      "source": [
        "df_AMITraining = pd.read_excel(\"/content/drive/MyDrive/AMI TrainingSet.xlsx\", header=0)\n",
        "df_AMITesting = pd.read_excel(\"/content/drive/MyDrive/AMI TestSet.xlsx\", header=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIs2gsOV97DS"
      },
      "outputs": [],
      "source": [
        "df_AMITraining"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_AMITesting"
      ],
      "metadata": {
        "id": "N1aFwIaxyvXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g3gIp9alMj7"
      },
      "outputs": [],
      "source": [
        "# Labels Encoding (LabelEncoder) TRAINING  choose manual or automatic annotation\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lbl_encoder = LabelEncoder()\n",
        "\n",
        "df_AMITraining['label'] = lbl_encoder.fit_transform(df_AMITraining['Asentiment'])\n",
        "\n",
        "df_AMITraining.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "2o5RR9xGliI6",
        "outputId": "5ba7ebe1-0802-44b4-8c1e-bd22e351e52d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      ID                                              Tweet Msentiment  \\\n",
              "327  328   علي فكره الاكتئاب ماله اي علاقه بالدين بنت خا...        POS   \n",
              "630  631                      ما اروع الارق بتاع الامتحانات        POS   \n",
              "501  502                                        الارق ملعون        NEG   \n",
              "346  347                         كلهم يعانون من ثنائي القطب        NEG   \n",
              "200  201        اللهم امانا لمن اعتاد القلق ونسي الطمانينه         POS   \n",
              "560  561  بعد مرور خمس سنوات من الارق اعلن ع هذا المنبر ...        POS   \n",
              "519  520                   الله ياخذ الارق والتفكير الزايد         NEG   \n",
              "587  588  ثاني اسوا شي بالحياه بعد انسداد الشهيه الارق ل...        NEG   \n",
              "57    58   اطمئن سيمضي القلق وستاتي الراحه بعد هذا الكم ...        POS   \n",
              "160  161   الضغط النفسي يولد الانفجار علي شيء اسخف من ال...        NEG   \n",
              "\n",
              "    Asentiment  label  \n",
              "327        POS      1  \n",
              "630        POS      1  \n",
              "501        NEG      0  \n",
              "346        NEG      0  \n",
              "200        POS      1  \n",
              "560        POS      1  \n",
              "519        NEG      0  \n",
              "587        NEG      0  \n",
              "57         POS      1  \n",
              "160        NEG      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1a948856-bcf9-44d7-8fd4-8926037baca3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Msentiment</th>\n",
              "      <th>Asentiment</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>327</th>\n",
              "      <td>328</td>\n",
              "      <td>علي فكره الاكتئاب ماله اي علاقه بالدين بنت خا...</td>\n",
              "      <td>POS</td>\n",
              "      <td>POS</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630</th>\n",
              "      <td>631</td>\n",
              "      <td>ما اروع الارق بتاع الامتحانات</td>\n",
              "      <td>POS</td>\n",
              "      <td>POS</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>502</td>\n",
              "      <td>الارق ملعون</td>\n",
              "      <td>NEG</td>\n",
              "      <td>NEG</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>347</td>\n",
              "      <td>كلهم يعانون من ثنائي القطب</td>\n",
              "      <td>NEG</td>\n",
              "      <td>NEG</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>201</td>\n",
              "      <td>اللهم امانا لمن اعتاد القلق ونسي الطمانينه</td>\n",
              "      <td>POS</td>\n",
              "      <td>POS</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>560</th>\n",
              "      <td>561</td>\n",
              "      <td>بعد مرور خمس سنوات من الارق اعلن ع هذا المنبر ...</td>\n",
              "      <td>POS</td>\n",
              "      <td>POS</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>520</td>\n",
              "      <td>الله ياخذ الارق والتفكير الزايد</td>\n",
              "      <td>NEG</td>\n",
              "      <td>NEG</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>587</th>\n",
              "      <td>588</td>\n",
              "      <td>ثاني اسوا شي بالحياه بعد انسداد الشهيه الارق ل...</td>\n",
              "      <td>NEG</td>\n",
              "      <td>NEG</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>58</td>\n",
              "      <td>اطمئن سيمضي القلق وستاتي الراحه بعد هذا الكم ...</td>\n",
              "      <td>POS</td>\n",
              "      <td>POS</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>161</td>\n",
              "      <td>الضغط النفسي يولد الانفجار علي شيء اسخف من ال...</td>\n",
              "      <td>NEG</td>\n",
              "      <td>NEG</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a948856-bcf9-44d7-8fd4-8926037baca3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1a948856-bcf9-44d7-8fd4-8926037baca3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1a948856-bcf9-44d7-8fd4-8926037baca3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# Labels Encoding (LabelEncoder) TESTING\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lbl_encoder = LabelEncoder()\n",
        "\n",
        "df_AMITesting['label'] = lbl_encoder.fit_transform(df_AMITesting['Asentiment'])\n",
        "\n",
        "df_AMITesting.sample(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61QBIPKh-Iyj"
      },
      "outputs": [],
      "source": [
        "df_AMITraining['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IO4bW77fZLT"
      },
      "outputs": [],
      "source": [
        "# Clean the dataset from any unnecessary texts\n",
        "STOPWORDS_PATH = '/content/drive/MyDrive/arabic_stopwords.txt'\n",
        "with open(STOPWORDS_PATH, 'r') as f:\n",
        "  ar_stopwords = set([word.strip() for word in f.readlines()])\n",
        "\n",
        "from pyarabic.araby import *\n",
        "\n",
        "def clean_text(text):\n",
        "  def get_none_stopwords(word):\n",
        "    return word not in ar_stopwords\n",
        "\n",
        "  return ' '.join(tokenize(text,\n",
        "           conditions = [is_arabicrange, get_none_stopwords],\n",
        "           morphs = [strip_tashkeel, strip_harakat, strip_tatweel]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_AMITraining['processed_Tweet'] = df_AMITraining['Tweet'].apply(clean_text)\n",
        "df_AMITraining.head()"
      ],
      "metadata": {
        "id": "XVTBOR_6k0ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiaVAvTQ96K9"
      },
      "outputs": [],
      "source": [
        "df_AMITesting['processed_Tweet'] = df_AMITesting['Tweet'].apply(clean_text)\n",
        "df_AMITesting.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wxykFKxRkXi"
      },
      "source": [
        "### Functions and data spliting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lrze5Jc0hmyS",
        "outputId": "82931c5b-a546-4e73-8541-6287e44aa684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocb size is  12625\n"
          ]
        }
      ],
      "source": [
        "# Build the vocab\n",
        "base_dir = '/content/drive/MyDrive/ Benchmark Evaluation '\n",
        "\n",
        "words_set = set()\n",
        "\n",
        "clean_data = df_AMITraining['processed_Tweet'].tolist()\n",
        "for s in clean_data:\n",
        "    words_set.update(s.split())\n",
        "\n",
        "clean_data2 = df_AMITesting['processed_Tweet'].tolist()\n",
        "for s in clean_data2:\n",
        "    words_set.update(s.split())\n",
        "\n",
        "with open(base_dir+'vocab.txt', 'w') as f:\n",
        "    f.write('\\n'.join(list(words_set)))\n",
        "\n",
        "print(\"Vocb size is \", len(list(words_set)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-WeqiFJtmgx"
      },
      "outputs": [],
      "source": [
        "# confusion_matrix code\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import math\n",
        "import numpy as np\n",
        "def confusion_matrix_scorer(y, pred):\n",
        "\n",
        "      cm = confusion_matrix(y, pred)\n",
        "      print_statistics(cm)\n",
        "      plot_confusion_matrix(cm, classes=['Negative','Positive'],\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues)\n",
        "      #to add rule matrix\n",
        "      return {'tn': cm[0, 0], 'fp': cm[0, 1],\n",
        "              'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
        "\n",
        "\n",
        "def print_statistics(cm):\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    # TP\n",
        "    print(\"TP: \" + str(tp))\n",
        "    # TN\n",
        "    print(\"TN: \" + str(tn))\n",
        "    # FP\n",
        "    print(\"FP: \" + str(fp))\n",
        "    # FN\n",
        "    print(\"FN: \" + str(fn))\n",
        "    # TPR\n",
        "    recall = tp/(tp+fn)\n",
        "    print(\"TPR/recall: \" + str(recall))\n",
        "    # TNR\n",
        "    specificity = tn/(tn+fp)\n",
        "    print(\"TNR/specificity: \" + str(specificity))\n",
        "    # PPV\n",
        "    precision = tp/(tp+fp)\n",
        "    print(\"PPV/precision: \" + str(precision))\n",
        "    # NPV\n",
        "    npv = tn/(tn+fn)\n",
        "    print(\"NPV/negative predictive value: \" + str(npv))\n",
        "    # FNR\n",
        "    miss_rate = 1-recall\n",
        "    print(\"FNR/false negative rate: \" + str(miss_rate))\n",
        "    # FPR\n",
        "    fall_out = 1-specificity\n",
        "    print(\"FPR/false positive rate: \" + str(fall_out))\n",
        "    # FDR\n",
        "    fdr = 1-precision\n",
        "    print(\"FDR/false discovery rate: \" + str(fdr))\n",
        "    # FOR\n",
        "    fomr = 1-npv\n",
        "    print(\"FOR/false ommission rate: \" + str(fomr))\n",
        "    # F1\n",
        "    f1 = 2*((precision*recall)/(precision+recall))\n",
        "    print(\"F1 score: \" + str(f1))\n",
        "    # accuracy\n",
        "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "    print(\"Accuracy: \" + str(acc))\n",
        "    # Matthews correlation coefficient (MCC)\n",
        "    mcc = (tp*tn-fp*fn)/math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
        "    print(\"MCC/Matthews correlation coefficient: \" + str(mcc))\n",
        "    # Informedness or Bookmaker Informedness (BM)\n",
        "    bm = recall+specificity-1\n",
        "    print(\"BM/Bookmaker Informedness: \" + str(bm))\n",
        "    # Markedness (MK)\n",
        "    mk = precision+npv-1\n",
        "    print(\"MK/Markedness: \" + str(mk))\n",
        "    return fall_out, recall\n",
        "    # credit: https://github.com/scikit-learn/scikit-learn/blob/master/examples/model_selection/plot_confusion_matrix.py\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "\n",
        "    plt.title(title)\n",
        "    #plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    s = [['TN','FP'], ['FN', 'TP']]\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            plt.text(j,i, (str(s[i][j])+\" = \"+str(format(cm[i][j],fmt))),horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ikcnOD8YCiz"
      },
      "outputs": [],
      "source": [
        "# Split the dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(df_dataset['processed_Tweet'], df_dataset['label'], test_size=0.2, stratify=df_dataset['label'])\n",
        "#len(X_train), len(X_test)\n",
        "\n",
        "#sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.20, random_state=1000)\n",
        "X_train= df_AMITraining['processed_Tweet'].values # Manual or Automatic  Sentiment\n",
        "y_train = df_AMITraining['label'].values\n",
        "\n",
        "X_test = df_AMITesting['processed_Tweet'].values\n",
        "y_test = df_AMITesting['label'].values\n",
        "\n",
        "len(X_train), len(X_test)\n",
        "print('Training: ', X_train.shape)\n",
        "print('Testing: ', X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0I6G_F7LePR"
      },
      "source": [
        "## Traditional Word Representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJVMkMZ-Xn9E"
      },
      "source": [
        "### Bow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCKkTKF3YS6N"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "BOW =  CountVectorizer()\n",
        "X_train_BOW  = BOW .fit_transform(X_train)\n",
        "X_test_BOW  = BOW .transform(X_test)\n",
        "\n",
        "X_train_BOW.shape, X_test_BOW.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bow+ SVM"
      ],
      "metadata": {
        "id": "uYfby4Zp1D5X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnH5wBGgZBG2"
      },
      "outputs": [],
      "source": [
        "# training using Support Vector Machines\n",
        "from sklearn import svm\n",
        "svm_clf = svm.SVC()\n",
        "svm_clf.fit(X_train_BOW, y_train)\n",
        "\n",
        "# Performance on training data\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred_train = svm_clf.predict(X_train_BOW)\n",
        "print(classification_report(y_train, y_pred_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE0OjxKrZpq8"
      },
      "outputs": [],
      "source": [
        "# Performance on testing data\n",
        "y_pred_test = svm_clf.predict(X_test_BOW)\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkEmBa7oZ5Ds"
      },
      "outputs": [],
      "source": [
        "confusion_matrix_scorer(y_test, y_pred_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bow+ LR"
      ],
      "metadata": {
        "id": "ezb3hZOK1KER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training using Support Vector Machines\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "LR_clf = LogisticRegression(max_iter=1000)\n",
        "LR_clf.fit(X_train_BOW, y_train)\n",
        "\n",
        "# Performance on training data\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred_train = LR_clf.predict(X_train_BOW)\n",
        "print(classification_report(y_train, y_pred_train))"
      ],
      "metadata": {
        "id": "xi6Tzzt61e_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance on testing data\n",
        "y_pred_test = LR_clf.predict(X_test_BOW)\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "swh1jXeL1fUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix_scorer(y_test, y_pred_test)"
      ],
      "metadata": {
        "id": "516Yjcdf1fih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bow+ DT"
      ],
      "metadata": {
        "id": "-3mUbJ3H10WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training using DT\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "DT_clf = DecisionTreeClassifier()\n",
        "DT_clf.fit(X_train_BOW, y_train)\n",
        "\n",
        "# Performance on training data\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred_train = DT_clf.predict(X_train_BOW)\n",
        "print(classification_report(y_train, y_pred_train))"
      ],
      "metadata": {
        "id": "26I-57NX15xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance on testing data\n",
        "y_pred_test = DT_clf.predict(X_test_BOW)\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "Ex7mfI5t15-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix_scorer(y_test, y_pred_test)"
      ],
      "metadata": {
        "id": "2XZQM7BC16LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBR-eAEGWTXK"
      },
      "source": [
        "### TF/IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwWD6itU8-bh"
      },
      "outputs": [],
      "source": [
        "# Split the dataset\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(df_dataset['processed_Tweet'], df_dataset['label'], test_size=0.2, stratify=df_dataset['label'])\n",
        "#len(X_train), len(X_test)\n",
        "\n",
        "#sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.20, random_state=1000) # مهم\n",
        "X_train= df_AMITraining['processed_Tweet'].values\n",
        "y_train = df_AMITraining['label'].values\n",
        "\n",
        "X_test = df_AMITesting['processed_Tweet'].values\n",
        "y_test = df_AMITesting['label'].values\n",
        "\n",
        "len(X_train), len(X_test)\n",
        "print('Training: ', X_train.shape)\n",
        "print('Testing: ', X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1UJAOSOO1Bl"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "X_train_tfidf.shape, X_test_tfidf.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNyv_xrAf3Nv"
      },
      "source": [
        "#### TF/IDF with the SVM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6WWAHjS94Ag"
      },
      "outputs": [],
      "source": [
        "# training using Support Vector Machines\n",
        "from sklearn import svm\n",
        "\n",
        "svm_clf = svm.SVC()\n",
        "\n",
        "svm_clf.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Qh2A8fC-Tsj"
      },
      "outputs": [],
      "source": [
        "# Performance on training data\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred_train = svm_clf.predict(X_train_tfidf)\n",
        "print(classification_report(y_train, y_pred_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDg227F1-s_S"
      },
      "outputs": [],
      "source": [
        "# Performance on testing data\n",
        "\n",
        "y_pred_test = svm_clf.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5QcAOW8C8UX"
      },
      "outputs": [],
      "source": [
        "confusion_matrix_scorer(y_test, y_pred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jby6KlamBtsl"
      },
      "outputs": [],
      "source": [
        "# save trained SVM model\n",
        "import joblib\n",
        "base_dir = '/content/drive/MyDrive/Colab/NLP/'\n",
        "joblib.dump(svm_clf, base_dir+'tfidf_svm_clf.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TF/IDF with the LR Classifier"
      ],
      "metadata": {
        "id": "eYujZU_n9pt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training using LR\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "LR_clf = LogisticRegression(max_iter=1000)\n",
        "LR_clf.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "utyNVZkP9tLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance on training data\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred_train = LR_clf.predict(X_train_tfidf)\n",
        "print(classification_report(y_train, y_pred_train))"
      ],
      "metadata": {
        "id": "LgWU2U3G-H-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance on testing data\n",
        "y_pred_test = LR_clf.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "k-3ei7Ia-IRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix_scorer(y_test, y_pred_test)"
      ],
      "metadata": {
        "id": "FllpIJC4-jLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TF/IDF with the DT Classifier"
      ],
      "metadata": {
        "id": "QhPbUNDm9uRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training using DT\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "DT_clf = DecisionTreeClassifier()\n",
        "DT_clf.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "HfZBNvR7-JJn",
        "outputId": "5dd467d4-9fb4-4079-defb-9f1661dbcc69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance on training data\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred_train = DT_clf.predict(X_train_tfidf)\n",
        "print(classification_report(y_train, y_pred_train))"
      ],
      "metadata": {
        "id": "McrOHtMt-Jah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance on testing data\n",
        "y_pred_test = DT_clf.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "26yk8WBU-JtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix_scorer(y_test, y_pred_test)"
      ],
      "metadata": {
        "id": "KIwqWHHx-mc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWFIaJyEW3un"
      },
      "source": [
        "## Word Embeddings: Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJP4jSl_b--e"
      },
      "outputs": [],
      "source": [
        "# Download the Word2Vec models\n",
        "\n",
        "\n",
        "# using the following on the paper\n",
        "# for cbow\n",
        "!wget https://archive.org/download/aravec2.0/tweet_cbow_300.zip -O w2v_cbow_300.zip #3 use this in the paper\n",
        "!mkdir -p w2v_cbow_300\n",
        "!unzip -qq w2v_cbow_300.zip -d w2v_cbow_300\n",
        "# for SG\n",
        "#!wget https://archive.org/download/aravec2.0/tweets_sg_300.zip -O w2v_sg_300.zip #2 use this in the paper\n",
        "#!mkdir -p w2v_sg_300\n",
        "#!unzip -qq w2v_sg_300.zip -d w2v_sg_300"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####cow + SG"
      ],
      "metadata": {
        "id": "VqkZKfM9D4Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "def load_w2v(filepath,binary):\n",
        "    return KeyedVectors.load_word2vec_format(filepath, binary=binary)"
      ],
      "metadata": {
        "id": "NV6puGzLFkVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk import ngrams\n",
        "\n",
        "\n",
        "# for cbow\n",
        "w2v_model = gensim.models.Word2Vec.load('./w2v_cbow_300/tweets_cbow_300') # 3*\n",
        "#for SG\n",
        "#w2v_model = gensim.models.Word2Vec.load('./w2v_sg_300/tweets_sg_300') # 2*\n",
        "\n",
        "len(w2v_model.wv.index_to_key)"
      ],
      "metadata": {
        "id": "u_L7MTN7FDAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Replace the words in each text message with the learned word vector\n",
        "words = set(w2v_model.wv.index_to_key)\n",
        "X_train_w2v = np.array([np.array([w2v_model.wv[w] for w in l if w in words]) for l in X_train])\n",
        "X_test_w2v = np.array([np.array([w2v_model.wv[w] for w in l if w in words]) for l in X_test])"
      ],
      "metadata": {
        "id": "qAALx3HXGI-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average the word vectors for each sentence in the training data\n",
        "X_train_w2v_avg = []\n",
        "\n",
        "for v in X_train_w2v:\n",
        "  if v.size:\n",
        "    X_train_w2v_avg.append(v.mean(axis=0))\n",
        "  else:\n",
        "    X_train_w2v_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "# Average the word vectors for each sentence in the testing data\n",
        "X_test_w2v_avg = []\n",
        "\n",
        "for v in X_test_w2v:\n",
        "  if v.size:\n",
        "    X_test_w2v_avg.append(v.mean(axis=0))\n",
        "  else:\n",
        "    X_test_w2v_avg.append(np.zeros(100, dtype=float))"
      ],
      "metadata": {
        "id": "An-MrWZ5GwiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu9DuZ_ptfz2"
      },
      "source": [
        "#### Using AraVec CBoW model with the SVM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-zBjKpMhC4c"
      },
      "outputs": [],
      "source": [
        "# training using Support Vector Machines\n",
        "from sklearn import svm\n",
        "\n",
        "svm_clf = svm.SVC()\n",
        "\n",
        "svm_clf.fit(X_train_w2v_avg, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7M-zVophMTq"
      },
      "outputs": [],
      "source": [
        "# Performance on training data\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "y_pred_train = svm_clf.predict(X_train_w2v_avg)\n",
        "print(classification_report(y_train, y_pred_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytTVAFNruHpn"
      },
      "outputs": [],
      "source": [
        "# Performance on testing data\n",
        "y_pred_test = svm_clf.predict(X_test_w2v_avg)\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jCGvO0UeMwR"
      },
      "outputs": [],
      "source": [
        "confusion_matrix_scorer(y_test, y_pred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CNDQqk6uHpo"
      },
      "outputs": [],
      "source": [
        "# save trained SVM model\n",
        "import joblib\n",
        "joblib.dump(svm_clf, base_dir+'w2v_svm_clf.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfW9DOMMj5hl"
      },
      "source": [
        "## Language Model: BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzk-lrytb-LM"
      },
      "outputs": [],
      "source": [
        "df_train =  pd.read_excel(\"/content/drive/MyDrive/AMI TrainingSet.xlsx\", header=0)\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrhuT2vqo4TZ"
      },
      "outputs": [],
      "source": [
        "# Labels Encoding (LabelEncoder) TRAINING\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lbl_encoder = LabelEncoder()\n",
        "df_train['label'] = lbl_encoder.fit_transform(df_train['ASentiment '])\n",
        "df_train.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pZHVLYaXIL2"
      },
      "outputs": [],
      "source": [
        "# Labels Encoding (OneHotEncoder)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "enc_df = pd.DataFrame(encoder.fit_transform(df_train[['ASentiment ']]).toarray())\n",
        "classes_names = np.array(encoder.categories_).tolist()[0] # categories are list of arrays\n",
        "enc_df.columns = classes_names\n",
        "df_train = df_train.join(enc_df)\n",
        "\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5X-rey5pOPO"
      },
      "outputs": [],
      "source": [
        "df_AMITesting.drop(columns=[\"ID\",\"MSentiment\", 'ASentiment '])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8kB4qhpaucj"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_excel(\"/content/drive/MyDrive/AMI TestSet.xlsx\", header=0)\n",
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5_MduaSpsZ_"
      },
      "outputs": [],
      "source": [
        "# Labels Encoding (LabelEncoder) TRAINING\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lbl_encoder = LabelEncoder()\n",
        "df_test['label'] = lbl_encoder.fit_transform(df_test['ASentiment '])\n",
        "df_test.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSYzQOelpyeW"
      },
      "outputs": [],
      "source": [
        "df_AMITesting.drop(columns=[\"ID\",\"MSentiment\", 'ASentiment '])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1nLUrHIpbb5"
      },
      "outputs": [],
      "source": [
        "len(df_train), len(df_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJFgV6ELit7N"
      },
      "outputs": [],
      "source": [
        "# Labels Encoding (OneHotEncoder)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "enc_df = pd.DataFrame(encoder.fit_transform(df_test[['ASentiment ']]).toarray())\n",
        "classes_names = np.array(encoder.categories_).tolist()[0] # categories are list of arrays\n",
        "enc_df.columns = classes_names\n",
        "df_test = df_test.join(enc_df)\n",
        "\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_ta3DIE0DS9"
      },
      "outputs": [],
      "source": [
        "# Convert text into BERT word embeddings\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'running on {device} device')\n",
        "\n",
        "model_name = \"aubmindlab/bert-large-arabertv02-twitter\" #\"aubmindlab/bert-base-arabertv2\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "bert_model = AutoModel.from_pretrained(model_name).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMhkKO-d9-40"
      },
      "outputs": [],
      "source": [
        "def getBERTWordEmbeddings(df_tweets, bert_tokenizer, bert_model,\n",
        "                          max_length=128, batch_size=32,\n",
        "                          device='cuda', return_tensors='pt'):\n",
        "\n",
        "  batches_embeddings = []\n",
        "  batches_labels = []\n",
        "\n",
        "  # split all articles into patches\n",
        "  tweets = df_tweets.Tweet.values.tolist()\n",
        "  for i in tqdm(range(0, len(tweets), batch_size)):\n",
        "\n",
        "    # tokenize all the atweets in batch\n",
        "    tweets_tokens = bert_tokenizer(tweets[i:i+batch_size], padding = True,\n",
        "                                    max_length=max_length, truncation = True,\n",
        "                                    return_tensors=return_tensors)\n",
        "\n",
        "    # move tokens to device\n",
        "    if not device == 'cude':\n",
        "      tweets_tokens = {k:v.to(device) for k,v in tweets_tokens.items()}\n",
        "\n",
        "    # extract BERT embeddings\n",
        "    with torch.no_grad():\n",
        "      hidden_samples = bert_model(**tweets_tokens)\n",
        "\n",
        "    # extract [CLS] last hidden states as embeddings and move them to CPU memory\n",
        "    embeddings = hidden_samples.last_hidden_state[:,0,:]\n",
        "    batches_embeddings.append(embeddings.to('cpu'))\n",
        "\n",
        "    batches_labels.append(df_tweets[i:i+batch_size][classes_names])\n",
        "\n",
        "  return batches_embeddings, batches_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kdwak6g0TaA"
      },
      "outputs": [],
      "source": [
        "train_embeddings, train_labels = getBERTWordEmbeddings(df_train, bert_tokenizer, bert_model)\n",
        "test_embeddings, test_labels = getBERTWordEmbeddings(df_test, bert_tokenizer, bert_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnvM3VaTRidW"
      },
      "source": [
        "#### Using AraBERT with the SVM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTp19swZV-Zn"
      },
      "outputs": [],
      "source": [
        "# training using Support Vector Machines\n",
        "from sklearn import svm\n",
        "\n",
        "svm_clf = svm.SVC()\n",
        "\n",
        "X_train = torch.cat(train_embeddings)\n",
        "y_train = df_train.label\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "print('training completed..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txM-08v6V-Zv"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Performance on training data\n",
        "y_pred_train = svm_clf.predict(X_train)\n",
        "accuracy = accuracy_score(y_train, y_pred_train)\n",
        "print('Training Accuracy: ', \"%.2f\" % (accuracy*100))\n",
        "print(classification_report(y_train, y_pred_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaNAA8gjV-Zw"
      },
      "outputs": [],
      "source": [
        "# Performance on testing data\n",
        "X_test = torch.cat(test_embeddings)\n",
        "y_test = df_test.label\n",
        "\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Testing Accuracy: ', \"%.2f\" % (accuracy*100))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix_scorer(y_test, y_pred)"
      ],
      "metadata": {
        "id": "WtMY1kRSs0R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMJeemGvV-Zw"
      },
      "outputs": [],
      "source": [
        "# Get predictions on sample article\n",
        "\n",
        "sample_article = df_test.sample(n=1)\n",
        "\n",
        "embeddings,_ = getBERTWordEmbeddings(sample_article, bert_tokenizer, bert_model)\n",
        "y_pred = svm_clf.predict(torch.cat(embeddings))\n",
        "\n",
        "print()\n",
        "print(sample_article.Tweet.item())\n",
        "print('Actual Category:', sample_article.Msentiment.item())\n",
        "print('Predicted Category:', lbl_encoder.inverse_transform(y_pred)[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRes0fPgV-Zw"
      },
      "outputs": [],
      "source": [
        "# save trained SVM model\n",
        "import joblib\n",
        "base_dir = '/content/drive/MyDrive/Colab/NLP/'\n",
        "joblib.dump(svm_clf, base_dir+'arabert_svm_clf.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LSTM - Bi-LSTM - CNN with fastText"
      ],
      "metadata": {
        "id": "XO3u09KkUexs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "id": "C37_b9x6RrG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "97-PqS2fNkd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chakin # fast text embedding\n",
        "!pip install -q pyarabic"
      ],
      "metadata": {
        "id": "PlZTAK16Rzy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import collections\n",
        "nltk.download('punkt')\n",
        "from tqdm import tqdm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras import layers\n",
        "from keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.backend import clear_session\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import Dense, Dropout, Activation,LSTM,SpatialDropout1D\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D,MaxPooling1D"
      ],
      "metadata": {
        "id": "P7cdkHV9fTqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chakin\n",
        "chakin.search(lang='Arabic')"
      ],
      "metadata": {
        "id": "jN_9DA1kSBIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " chakin.download(number=0, save_dir='./') # select fastText(ar)"
      ],
      "metadata": {
        "id": "rIO1g3NzSTKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for embedding Layer\n",
        "from gensim.models import KeyedVectors\n",
        "def load_w2v(filepath,binary):\n",
        "    return KeyedVectors.load_word2vec_format(filepath, binary=binary)\n"
      ],
      "metadata": {
        "id": "8C9dUVT9SlQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for fast text embedding 300  dim\n",
        "w2v = load_w2v(\"./cc.ar.300.vec.gz\", binary=False) # takes ~10 mins to load\n",
        "print(len(w2v.key_to_index))#2000000"
      ],
      "metadata": {
        "id": "VB_h2lBBOpze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    accuracy = history.history['accuracy']\n",
        "    val_accuracy = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs = range(1,len(accuracy) + 1)\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.figure(1)\n",
        "    plt.plot(epochs, accuracy, 'b', label='Training accuracy')\n",
        "    plt.plot(epochs, val_accuracy, 'g', label='Validation accuracy')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    plt.figure(2)\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'g', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "v_vUSqviaKmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#exploratory analysis on the data\n",
        "maxlen = 0\n",
        "minlen=10000000\n",
        "word_freqs = collections.Counter()\n",
        "num_recs = 0\n",
        "\n",
        "numberOfRow=int(len(df_AMITraining.index))\n",
        "for ind in range (0,numberOfRow):\n",
        "    sentence=df_AMITraining['processed_Tweet'][ind]\n",
        "\n",
        "    words =nltk.word_tokenize(sentence)\n",
        "\n",
        "    if (len(words) > maxlen):\n",
        "       maxlen = len(words)\n",
        "    if len(words) < minlen:\n",
        "       minlen = len(words)\n",
        "    for word in words:\n",
        "        word_freqs[word] += 1\n",
        "    num_recs +=1\n",
        "print(\"maxlen\", maxlen)\n",
        "print(\"minlen\", minlen)\n",
        "print(\"len(word_freqs)\",len(word_freqs))\n",
        "print()"
      ],
      "metadata": {
        "id": "sDzgqznbS5Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data set for DNN model\n",
        "X_train_DNN= df_AMITraining['processed_Tweet'].values # Manual or Automatic  Sentiment\n",
        "y_train_DNN = df_AMITraining['label'].values\n",
        "\n",
        "X_test_DNN = df_AMITesting['processed_Tweet'].values\n",
        "y_test_DNN= df_AMITesting['label'].values\n",
        "\n",
        "len(X_train_DNN), len(X_test)\n",
        "print('Training: ', X_train_DNN.shape)\n",
        "print('Testing: ', X_test_DNN.shape)\n",
        "\n",
        "\n",
        "# Tokenize and preprocess the data\n",
        "num_words = 20000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train_DNN)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index)+2 # pad  + OOV\n",
        "max_len = 50\n",
        "X_train_DNN= pad_sequences(tokenizer.texts_to_sequences(X_train_DNN), maxlen=max_len)\n",
        "X_test_DNN= pad_sequences(tokenizer.texts_to_sequences(X_test_DNN), maxlen=max_len)\n",
        "\n",
        "X_train_DNN, X_valid_DNN, y_train_DNN, y_valid_DNN = train_test_split(X_train_DNN, y_train_DNN, test_size=0.1, random_state=42)\n",
        "\n",
        "print(\"Training:\", len(X_train_DNN))\n",
        "print(\"validation: \", len(X_valid_DNN))"
      ],
      "metadata": {
        "id": "Z02Or_27TjKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_NB_WORDS = len(w2v.key_to_index)\n",
        "EMBEDDING_DIM = 300 #  FastText"
      ],
      "metadata": {
        "id": "lxthtYX-Sq1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create embedding layer -  ِfastText embedding 300  dim\n",
        "def create_embedding_matrix(word_index):\n",
        "    nb_words = min(MAX_NB_WORDS, len(word_index))+2\n",
        "    #nb_words=len(tokenizer.word_index)\n",
        "    embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "    for word, ii in word_index.items():\n",
        "        if word in w2v.key_to_index:  #w2v.key_to_index\n",
        "            embedding_matrix[ii] = w2v.word_vec(word)\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix = create_embedding_matrix(word_index)\n",
        "print(embedding_matrix.shape)\n",
        "print(embedding_matrix)"
      ],
      "metadata": {
        "id": "TPbU6dxcO_Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LSTM"
      ],
      "metadata": {
        "id": "DfFMN2weq1lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model\n",
        "#LSTM\n",
        "\n",
        "EMBEDDING_SIZE = 300\n",
        "HIDDEN_LAYER_SIZE = 64\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 5\n",
        "clear_session()\n",
        "model = Sequential()\n",
        "#model.add(Embedding(num_words, EMBEDDING_SIZE,input_length=max_len))\n",
        "model.add(layers.Embedding(vocab_size, EMBEDDING_SIZE,\n",
        "                          weights=[embedding_matrix],\n",
        "                          input_length=max_len, trainable=True))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "Q8L6rftkU5b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_DNN, y_train_DNN,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_valid_DNN, y_valid_DNN),\n",
        "                    batch_size=BATCH_SIZE)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_train_DNN, y_train_DNN, verbose=True ,batch_size=BATCH_SIZE)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "\n",
        "loss_val, accuracy_val= model.evaluate(X_valid_DNN, y_valid_DNN, verbose=True,batch_size=BATCH_SIZE)\n",
        "print(\"validation  Accuracy:  {:.4f}\".format(accuracy_val))"
      ],
      "metadata": {
        "id": "am6cgmNEZYs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history)"
      ],
      "metadata": {
        "id": "AOcqPk7LaOKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, acc = model.evaluate(X_test_DNN, y_test_DNN , batch_size=BATCH_SIZE)\n",
        "print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))\n",
        "y_pred_LSTM=[]\n",
        "for idx in range(0,len(X_test_DNN)):\n",
        "  xtest = X_test_DNN[idx].reshape(1,max_len)\n",
        "  ylabel = y_test_DNN[idx]\n",
        "  ypred = model.predict(xtest)[0][0]\n",
        "  y_pred_LSTM.append(\"%.0f\" % (ypred) ) # for convision matrix\n",
        "# str to int\n",
        "for i in range(len(y_pred_LSTM)):\n",
        "    y_pred_LSTM[i]=int(y_pred_LSTM[i])"
      ],
      "metadata": {
        "id": "pR0yhGlvi0Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(y_pred_LSTM)\n",
        "#print( y_test_DNN)\n",
        "confusion_matrix_scorer( y_test_DNN,y_pred_LSTM)"
      ],
      "metadata": {
        "id": "LOtONZovlamX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using bi-LSTM with fastText"
      ],
      "metadata": {
        "id": "tjaUo95xUnnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create Bidirectional with fast text embedding\n",
        "\n",
        "EMBEDDING_SIZE = 300\n",
        "HIDDEN_LAYER_SIZE = 32\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "\n",
        "clear_session()\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, EMBEDDING_SIZE,\n",
        "                          weights=[embedding_matrix],\n",
        "                          input_length=max_len,\n",
        "                          trainable=True))\n",
        "model.add(layers.Bidirectional(layers.LSTM(HIDDEN_LAYER_SIZE, dropout=0.2,\n",
        "                                           recurrent_dropout=0.2,\n",
        "                                           return_sequences=True)))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "4tjCkS29U7Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_DNN, y_train_DNN,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_valid_DNN, y_valid_DNN),\n",
        "                    batch_size=BATCH_SIZE)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_train_DNN, y_train_DNN, verbose=True ,batch_size=BATCH_SIZE)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "\n",
        "loss_val, accuracy_val= model.evaluate(X_valid_DNN, y_valid_DNN, verbose=True,batch_size=BATCH_SIZE)\n",
        "print(\"validation  Accuracy:  {:.4f}\".format(accuracy_val))"
      ],
      "metadata": {
        "id": "ldeIA01xU7NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, acc = model.evaluate(X_test_DNN, y_test_DNN , batch_size=BATCH_SIZE)\n",
        "print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))\n",
        "y_pred_BiLSTM=[]\n",
        "for idx in range(0,len(X_test_DNN)):\n",
        "  xtest = X_test_DNN[idx].reshape(1,max_len)\n",
        "  ylabel = y_test_DNN[idx]\n",
        "  ypred = model.predict(xtest)[0][0]\n",
        "  y_pred_BiLSTM.append(\"%.0f\" % (ypred) ) # for convision matrix\n",
        "# STR to int\n",
        "for i in range(len(y_pred_BiLSTM)):\n",
        "    y_pred_BiLSTM[i]=int(y_pred_BiLSTM[i])"
      ],
      "metadata": {
        "id": "KXkIfZ8dU7Rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(y_pred_BiLSTM)\n",
        "#print( y_test_DNN)\n",
        "confusion_matrix_scorer( y_test_DNN,y_pred_BiLSTM)"
      ],
      "metadata": {
        "id": "wFe-cu85xBGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using CNN with fastText"
      ],
      "metadata": {
        "id": "o8UPOR97UqNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set parameters:\n",
        "BATCH_SIZE= 32\n",
        "embedding_dims = 300 # 50 without fastText\n",
        "filters = 250\n",
        "kernel_size = 3\n",
        "hidden_dims = 250\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "clear_session()\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "'''model.add(Embedding(vocab_size,embedding_dims, input_length=MAX_SENTENCE_LENGTH))'''\n",
        "\n",
        "model.add(layers.Embedding(vocab_size, embedding_dims,\n",
        "                          weights=[embedding_matrix],\n",
        "                          input_length= max_len,\n",
        "                          trainable=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# we add a Convolution1D, which will learn filters\n",
        "# word group filters of size filter_length:\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "# we use max pooling:\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "8Gh3kVOv0hUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_DNN, y_train_DNN,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_valid_DNN, y_valid_DNN),\n",
        "                    batch_size=BATCH_SIZE)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_train_DNN, y_train_DNN, verbose=True ,batch_size=BATCH_SIZE)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "\n",
        "loss_val, accuracy_val= model.evaluate(X_valid_DNN, y_valid_DNN, verbose=True,batch_size=BATCH_SIZE)\n",
        "print(\"validation  Accuracy:  {:.4f}\".format(accuracy_val))"
      ],
      "metadata": {
        "id": "SdDwaXStyAXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, acc = model.evaluate(X_test_DNN, y_test_DNN , batch_size=BATCH_SIZE)\n",
        "print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))\n",
        "y_pred_CNN=[]\n",
        "for idx in range(0,len(X_test_DNN)):\n",
        "  xtest = X_test_DNN[idx].reshape(1,max_len) # max len for tweet\n",
        "  ylabel = y_test_DNN[idx]\n",
        "  ypred = model.predict(xtest)[0][0]\n",
        "  y_pred_CNN.append(\"%.0f\" % (ypred) ) # for convision matrix\n",
        "# string to int\n",
        "for i in range(len(y_pred_CNN)):\n",
        "    y_pred_CNN[i]=int(y_pred_CNN[i])"
      ],
      "metadata": {
        "id": "shUMyT9zU8Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print( y_pred_CNN) # predected class\n",
        "#print( y_test_DNN) # actual label\n",
        "confusion_matrix_scorer( y_test_DNN, y_pred_CNN)"
      ],
      "metadata": {
        "id": "m3zk3WfWU8w1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1wxykFKxRkXi",
        "eJVMkMZ-Xn9E",
        "uYfby4Zp1D5X",
        "ezb3hZOK1KER",
        "-3mUbJ3H10WT",
        "UBR-eAEGWTXK",
        "ziwj67aHgqFD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}